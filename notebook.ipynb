{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Conforme explicado anteriormente, o RAG é um conjunto de técnicas que visam expandir o contexto de uma LLM, adaptando-a a problemas e contextos específicos. Muitas implementações estão feitas na biblioteca [Langchain](https://www.langchain.com/) (leia a documentação!) e partiremos dela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de bibliotecas\n",
    "!pip install langchain chromadb langchain-community ibm-watsonx-ai langchain-ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credenciais\n",
    "\n",
    "Todo o setup está sendo feito com base no arquivo `.env` que está no repositório. Essa configuração inicial pode ser revisada com mais calma [aqui](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/c3dbf23a-9a56-4c4b-8ce5-5707828fc981?context=wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.environ.get('WATSONX_APIKEY'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste do watsonx\n",
    "\n",
    "Agora, faremos o setup dos parâmetros. Eles podem ser observados no Prompt Lab. O mais importante é o `DECODING_METHOD`, que aceita as opções greedy e sample:\n",
    "\n",
    "- Greedy: sempre pega o token com a maior probabilidade, tendendo a ser mais **estável**\n",
    "- Sampling: insere aleatoriedade no processo de geração, dando uma chance maior para que tokens com a menor probabilidade sejam escolhidos\n",
    "\n",
    "`MAX_NEW_TOKENS`: quantidade máxima de novos tokens a serem gerados\n",
    "\n",
    "`MIN_NEW_TOKENS`: quantidade mínima de novos tokens a serem gerados\n",
    "\n",
    "A partir da escolha do método de sampling, outras variáveis passam a ser utilizadas:\n",
    "\n",
    "`TEMPERATURE`: pode ser entendida como a \"criatividade\" da geração. Quanto maior a temperatura, maior a probabilidade de escolher tokens com uma menor probabilidade. Varia de 0 a 2.\n",
    "\n",
    "`TOP_K` e `TOP_P`: controlam como os tokens são selecionados. Os valores típicos são de 50 e 1, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.TOP_P: 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos fazer a instanciação do modelo em si. Primeiramente, vamos listar os modelos disponíveis, para consulta e lembrando que os mesmos podem ser testados no Prompt Lab de forma rápida e prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2']\n"
     ]
    }
   ],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "import os\n",
    "\n",
    "flan_t5_model = Model(\n",
    "    model_id=ModelTypes.FLAN_T5_XXL,\n",
    "    credentials=credentials, # Credenciais definidas no início do notebook\n",
    "    project_id=os.environ.get('WATSONX_PROJECT_ID'), # ID do projeto no CP4D\n",
    "    params=parameters)\n",
    "\n",
    "# flan_t5_ul2_model = Model(\n",
    "#     model_id=\"google/flan-t5-ul2\",\n",
    "#     credentials=credentials,\n",
    "#     project_id=os.environ.get('WATSONX_PROJECT_ID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O LangChain funciona com base na ideia da montagem de cadeias de informação. Com isso, vamos testar o modelo carregado a partir do watsonx com a seguinte estrutura:\n",
    "\n",
    "1. PromptTemplate: um esqueleto de prompt que será enviado para o LLM\n",
    "2. SimpleSequentialChain: basicamente vai construir a entrada a partir do template e da entrada\n",
    "\n",
    "Para um teste simples, vamos passar um review para que o modelo classifique o mesmo como positivo ou negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"Based on the following review, categorize the sentiment as 'positive' or 'negative':\n",
    "{review} \n",
    "Sentiment: \"\n",
    "\"\"\"\n",
    "\n",
    "# Aqui, 'review' é a entrada (que será passada mais adiante)\n",
    "prompt_test = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "prompt_to_flan_t5 = LLMChain(llm=flan_t5_model.to_langchain(), # O método '.to_langchain()' é usado para transformar o modelo para um formato reconhecido pelas demais funções do langchain\n",
    "                             prompt=prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Montagem da chain\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_flan_t5], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mpositive\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\"\n",
    "\n",
    "qa.run(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solução de RAG em cima de uma base de conhecimento\n",
    "\n",
    "[Referência](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "\n",
    "**Como criar um sistema que consiga construir respostas com base em um contexto específico?**\n",
    "\n",
    "Para exemplificar o problema, vamos partir de um conjunto de artigos do IEEE. Todos tem um esqueleto básico, mas possuem formatações completamente diferentes entre si. Após extrair os textos desses artigos, vamos armazená-los em uma base de alguma forma para construir a resposta com base neles.\n",
    "\n",
    "No entanto, não é possível colocar todos os artigos dentro da janela de contexto/prompt do LLM. Com isso, um processo de extração das passagens mais importantes também deve ser feito, de forma que o LLM receba a pergunta e as parcelas mais importantes da base de conhecimento para gerar sua resposta. A estrutura ficará dessa forma:\n",
    "\n",
    "<img src=\"images/rag_retrieval_generation.png\" height=\"400px\">\n",
    "\n",
    "1. A Pergunta entra na base, onde um processo de extração encontra os fragmentos mais pertinentes para responder a pergunta;\n",
    "2. Os fragmentos e a questão são inseridas no prompt;\n",
    "3. O prompt é enviado para o LLM e uma resposta é dada.\n",
    "\n",
    "**Como construir esse banco de informações?**\n",
    "\n",
    "Esse banco de informações receberá (no nosso caso) PDFs, mas o langchain possui [métodos nativos](https://python.langchain.com/docs/modules/data_connection/document_loaders/) para CSVs, HTML, JSON, Markdown, PDF e outros.\n",
    "\n",
    "Esses documentos são segmentados em parcelas menores (e configuráveis). Dessa forma, o fatiamento permite que os documentos estejam separados em parcelas de mesmo tamanho.\n",
    "\n",
    "Após esse _split_, os documentos são vetorizados. Assim, eles podem ser comparados matematicamente entre si e com a pergunta por meio de operações semelhantes a operações euclidianas.\n",
    "\n",
    "Por fim, esses vetores são armazenados em uma base de dados chamada de **Vector store** ou **vector DB**.\n",
    "\n",
    "<img src=\"images/vector_db.png\" height=\"400px\">\n",
    "\n",
    "## Carregamento dos dados no VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\35-GHz Barium Hexaferrite or PDMS Composite-Based Millimeter-Wave Circulators for 5G Applications.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [00:00<00:24,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\A comprehensive survey on machine learning for networking - Evolution, applications and research opportunities.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/34 [00:04<01:14,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\A gated dilated causal convolution based encoder-decoder for network traffic forecasting.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/34 [00:04<00:45,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\A network traffic forecasting method based on SA optimized ARIMA-BP neural network.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4/34 [00:06<00:54,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\A Self-Adaptive Deep Learning-Based System for Anomaly Detection in 5G Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 5/34 [00:08<00:50,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\A Survey on Big Data for Network Traffic Monitoring and Analysis.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 6/34 [00:09<00:45,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Analyzing and modeling spatio-temporal dependence of cellular traffic at city scale.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 7/34 [00:12<00:56,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Application of Machine Learning in Wireless Networks - Key Techniques and Open Issues.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 8/34 [00:15<00:57,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Backhauling 5G Small Cells - A Radio Resource Management Perspective.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 9/34 [00:15<00:39,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Beyond Moran’s I - Testing for spatial dependence based on the spatial autoregressive model.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 10/34 [00:18<00:47,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Big data-driven optimization for mobile networks toward 5G.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 11/34 [00:19<00:38,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Characterizing the spatio-temporal inhomogeneity of mobile traffic in large-scale cellular data networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 13/34 [00:20<00:19,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Clustering to Enhance Network Traffic Forecasting.pdf\n",
      "data\\Deep learning in mobile and wireless networking - A survey.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 14/34 [00:22<00:30,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Deploying Virtual Network Functions With Non-Uniform Models in Tree-Structured Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 15/34 [00:24<00:30,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Design of mm-Wave Slow-Wave-Coupled Coplanar Waveguides.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 16/34 [00:27<00:37,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Exploring Network-Wide Flow Data With Flowyager.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 18/34 [00:51<01:34,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Generative-Adversarial-Network-Based wireless channel modeling - Challenges and opportunities.pdf\n",
      "data\\Hierarchical, virtualised and distributed intelligence 5G architecture for low-latency and secure applications.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 19/34 [00:51<01:04,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Improving traffic forecasting for 5G core network scalability - A machine learning approach.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 20/34 [00:52<00:46,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Long-term mobile traffic forecasting using deep spatio-temporal neural networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 21/34 [00:59<00:58,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Massive MIMO CSI Feedback Based on Generative Adversarial Network.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 22/34 [01:00<00:38,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Orchestrating Virtualized Network Functions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 23/34 [01:02<00:32,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Representational power of Restricted Boltzmann Machines and Deep Belief Networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 24/34 [01:02<00:21,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Resource Allocation in NFV- A Comprehensive Survey.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 25/34 [01:05<00:22,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Resource Sharing Efficiency in Network_Slicing.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 27/34 [01:12<00:18,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Spatial modeling of the traffic density in cellular networks.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 28/34 [01:12<00:10,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Spatial Traffic Distribution In Cellular Networks.pdf\n",
      "data\\Spatio-temporal analysis and prediction of cellular traffic in metropolis.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 29/34 [01:13<00:08,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Spatiotemporal modeling and prediction in cellular networks - A big data enabled deep learning approach.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 30/34 [01:13<00:04,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\TANGO - Traffic-Aware Network Planning and Green Operation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 31/34 [01:14<00:02,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Time4 - Time for SDN.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 33/34 [01:16<00:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Towards Supporting Intelligence in 5G-6G Core.pdf\n",
      "data\\Understanding Mobile Traffic Patterns of Large Scale Cellular Towers in Urban Environment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [01:19<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "# Onde os arquivos estão salvos\n",
    "files_dir = 'data'\n",
    "\n",
    "# Use a função glob para listar todos os arquivos PDF no diretório\n",
    "pdf_files = glob(join(files_dir, '*.pdf'))\n",
    "\n",
    "docs=[]\n",
    "\n",
    "for arquivo in tqdm(pdf_files):\n",
    "    print(arquivo)\n",
    "\n",
    "    loader = PyPDFLoader(arquivo)\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos montar o Splitter, que fará a separação dos arquivos em **chunks**. Vamos usasr o [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter).\n",
    "\n",
    "Os chunks terão um tamanho de 1500 caracteres e com um overlap de 150, para evitar que assuntos sejam abruptamente separados.\n",
    "\n",
    "<img src=\"images/chunking.jpg\" height=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# O splitter fará o \"fatiamento\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos construir o vetor a partir dos chunks usando o [chromadb](https://www.trychroma.com/), um vectordb que vai armazenar os vetores criados a partir dos **embeddings** dos chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# Esses embeddings podem ser testados em casos específicos\n",
    "# Para mais informações: https://www.sbert.net/\n",
    "ef = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "#ef = embedding_functions.SentenceTransformerEmbeddingFunction('multi-qa-mpnet-base-dot-v1')\n",
    "#ef = embedding_functions.SentenceTransformerEmbeddingFunction('multi-qa-mpnet-base-dot-v1')\n",
    "#ef = SentenceTransformerEmbeddings(model_name='distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Onde o vectorDB será salvo\n",
    "persist_directory = 'chroma_full/'\n",
    "\n",
    "# Montagem\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=ef,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo um teste simples de retrieval\n",
    "docs = vectordb.similarity_search('What is Nnwdaf_AnalyticsSubscriptionservice?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual sessions. The UPF processes and forwards user data\n",
      "and is controlled by the SMF. In addition, the UPF connects to\n",
      "external IP networks to act as anchor points, hiding mobility.\n",
      "The Uniﬁed Data Management Function (UDM) accesses user\n",
      "subscription data stored in the Uniﬁed Data Repository (UDR),a database containing network/user policies and associated\n",
      "data. Finally, the Authentication Server Function provides au-\n",
      "thentication services for a speciﬁc device, utilizing credentials\n",
      "from the UDM [12].\n",
      "As an underlying function solely responsible for data ana-\n",
      "lytics and network learning, the NWDAF represents operator-\n",
      "managed network analytics as a logical function [2]. The\n",
      "NWDAF provides slice-speciﬁc network data analytics to any\n",
      "given NF. As well, the NWDAF provides network analytics\n",
      "information to NFs on a network slice instance level. The\n",
      "function also notiﬁes NFs with slice-speciﬁc network status\n",
      "analytic information for any that are subscribed to it. NFs may\n",
      "also collect network status analytic information directly from\n",
      "the NWDAF. In the 5G Core, both the Policy Control Function\n",
      "(PCF) and the Network Slice Selection Function (NSSF) are\n",
      "consumers of network analytics. The PCF may use that data\n",
      "in its policy decisions, and the NSSF may use the load-level\n",
      "information provided by the NWDAF for slice selection.\n",
      "B. NWDAF\n",
      "The NWDAF architecture is designed to aid policy and\n",
      "decision-making for NFs in the control plane and supports\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração do prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you can't make a answer with context, just say that you don't know, don't try to make up an answer. Give the document name from where the information is extracted\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de extração\n",
    "\n",
    "Existem [diferentes formas](https://python.langchain.com/docs/modules/data_connection/retrievers/) de fazer a extração a partir da base de conhecimento. Aqui, usaremos um `LLMChainExtractor` que vai extrair dos vetores somente as parcelas mais úteis de cada um.\n",
    "\n",
    "O `search_type='mmr'` garante um pouco mais de diversidade nas parcelas extraídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Wrap our vectorstore\n",
    "compressor = LLMChainExtractor.from_llm(llm=flan_t5_model.to_langchain())  # uses an LLMChain to extract from each document only the statements that are relevant to the query.\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type='mmr')  # Retriever method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montagem da Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=flan_t5_model.to_langchain(),  # O modelo usado\n",
    "#     retriever=vectordb.as_retriever(),  # O retriever usado\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},  # O prompt\n",
    "# )\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=flan_t5_model.to_langchain(),  # O modelo usado\n",
    "    retriever=compression_retriever,  # O retriever usado\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},  # O prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Hadoop Distribute File System (HDFS)\n",
      "\n",
      "\n",
      "Source:  data\\Hierarchical, virtualised and distributed intelligence 5G architecture for low-latency and secure applications.pdf\n",
      "Page:  3\n",
      "\n",
      "\n",
      "Source:  data\\Towards Supporting Intelligence in 5G-6G Core.pdf\n",
      "Page:  1\n",
      "\n",
      "\n",
      "Source:  data\\Deep learning in mobile and wireless networking - A survey.pdf\n",
      "Page:  20\n",
      "\n",
      "\n",
      "Source:  data\\Hierarchical, virtualised and distributed intelligence 5G architecture for low-latency and secure applications.pdf\n",
      "Page:  4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main types of open source databases used in 5G infrastructure?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print('\\n\\n')\n",
    "print(result[\"result\"])  # Printando a saída\n",
    "print('\\n')\n",
    "\n",
    "for source in result['source_documents']:  # Como os documentos base estão na resposta, podemos saber exatamente a partir de qual documento e página as respostas estão sendo geradas\n",
    "    print ('Source: ', source.metadata['source'].split('/')[-1])\n",
    "    print('Page: ', source.metadata['page'])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watsonxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
